# helens项目可能会涉及到的问题

#### 项目介绍

这个项目是在学习网络编程的时候开发的一个高性能服务器框架，服务器的网络模型是基于协程开发的，IO处理基于epoll使用了IO多路复用技术，可以处理多个客户端的http请求。项目工作分为两部分，一部分是服务器框架的基本模块的封装，比如日志模块，配置模块、网络模块、socket模块、线程模块等一些基本系统的搭建。另一部分是为了提高服务器性能做的一些优化，比如协程模块，协程调度模块、io协程调度模块。最后此项目也是成功部署再华为云服务器上，并且基于此框架实现了一个聊天室的项目，聊天室的项目参与不多，基本是负责和开发此聊天室项目的同学进行联调，对服务器框架进行一些完善，包括添加了长连接模式，添加了数据库的接口实现等。

最后使用apache的ab压测工具在相同的机器环境下对HTTP的访问做了压力测试，对页面访问和nginx对比，性能可能要差%5左右，和libevent相比性能基本持平。ab -n

在开发此项目期间参考了一些开源框架的实现？（这块再想一下）

sylar



**日志模块：**支持流式日志风格写日志和格式化风格写日志，支持日志格式自定义，日志级别，多日志分离等等功能，支持时间,线程id,线程名称,日志级别,日志名称,文件名,行号等内容的自由配置。**没有考虑到异步日志的实现**

**配置模块：**用过spring，使用了他的约定大于配置的思想，定义即可使用，不需要单独解析。使用ini文件和yaml作为配置内容，支持多级别日志，支持stl容器，支持自定义类型（需要实现序列化和反序列化方法）

**线程模块：**封装了pthread的常用功能，Thread,Semaphore,Mutex,RWMutex,Spinlock等对象，可以方便开发中对线程日常使用。没有使用c++11的thread，因为thread也是基于pthread实现的而且c++11里面没有提供读写互斥量，比如RWMutex，Spinlock等。在高并发场景下，这些对象需要经常用到，所以选择自己进行封装pthread。

**协程模块：**用户态的线程，相当于线程中的线程，更轻量级。后续配置了socket hook模块，可以把复杂的异步调用封装成同步操作，降低业务逻辑的编写复杂度。目前该协程是基于ucontext_t实现的，参考了一些使用boost.context里面的fcontext_t的实现，因为基于ucontext_t的实现比较少。

**协程调度模块：**协程调度器。管理协程的调度，内部封装线程池，支持协程在多线程切换，也可以指定协程在固定的线程中执行。是一个M-N的协程调度模型，N个线程，M个协程。重复利用每一个协程。

**IO协程调度模块：**继承了协程调度器，封装了epoll。

**Hook模块**：hook了一些和socket io相关的底层API，包括read、write等。hook的开启控制是线程粒度的。可以自己控制是否开启。通过hook模块，可以使得一些不具备异步功能的API，展现出异步的性能，比如mysql

**Socket模块：**封装了socket模块，提供了所有的socket API功能，封装了地地址类。提供了域名，ip解析功能

**ByteArray序列化模块：**二进制序列化模块。提供对二进制数据的常用操作，读写基础类型int_8、int_32等，支持string镀锡，支持字节序转换、支持序列化到文件和从文件反序列化等功能。

**TCPserver模块：**基于 Socket类，封装了一个通用的TCPServer的服务器类，提供简单的API，使用便捷，可以快速绑定一个或者多个地址，启动服务，监听端口、accept连接、处理socket等功能。具体业务功能继承此类就可以快速实现。

**HTTP模块：**采用Ragel（有限状态机，性能媲美汇编），实现了HTTP/1.1的简单协议实现（request和response）和uri解析。实现了HttpConnection和HTTPsession。基于TCPServer实现了HTTPServer，提供完整的HTTP客户端API请求功能，HTTP基础API服务器功能。

##### **项目中的难点**

该服务器框架主要分为两个部分：第一部分关于服务器基本系统的搭建，难点在于技术的理解和选型，以及一些开源的框架调整后应用到项目中。第二部分的难点主要集中在提高服务器性能方面，包括找到服务器的性能瓶颈，如何突破？（这块在想一下）：协程调试上、并发上，展现异步性能，一核有时候会报错

##### 项目中遇到的困难？如何解决的？

1、一方面是对不同技术的理解和选型，难以选出最合适的技术框架。这部分的话基本上是阅读和对比各个开源项目的代码和技术文档，然后也去搜索一些技术对比的论文、博客去看，如果没有相关资料，尝试联系一些提供开源代码的大佬们。

2、编程期间遇到的困难，在代码编写过程中总会出现各种各样的bug。这也是我为什么做日志模块的一个很大的驱动力。一般通过日志定位bug，然后推断bug的原因尝试修复，如果自己看不出来可能就到网上看一下此类问题的解决办法，或者到一些知名论坛stackoverflow、csdn或者找同学等。

#### 针对项目做了哪些优化？

1、程序本身

- 减少程序等待IO的事件：非阻塞+IO复用
- 涉及协程相关的优化（这块再想一下，仔细展开）
  - 线程池
- 锁相关
  - 优化锁的使用，尽量减少临界区（线程池的时候）
- hook模块
  - 通过hook模块，可以使得一些不具备异步功能的API，展现出异步的性能，比如mysql

2、系统参数调优（这块再搜集一下）

- 最大文件描述符（用户级和系统级）
- tcp连接的参数（半连接/连接队列的长度、TCP sync cookies）



#### 项目用到了哪些设计模式？（这块要收集一下）

单例：封装了单例工具类

观察者模式：



#### 面向对象特性再项目中的体现

封装，继承，多态

封装：在项⽬中将各个模块使⽤类进⾏封装，⽐如连接httpconnection类来封装，日志就⽤ log 类来封装，将类的属性私有化，⽐如请求的解析状态，并且对外的接⼝设置为公有，⽐如连接的重置，不对外暴露⾃身的私有⽅法，⽐如读写的回调函数等。还有⼀个就是，项⽬中每个模块都使⽤了各⾃的命名空间进行封装，避免了命名冲突或者名字污染。**还有就是每个类都会定义一个share_ptr<类名>的变量名，这样方便用智能指针进行管理，防止不必要的内存泄漏**

继承：主要是对工具类的继承（单例模式），项目中多个地方到 noncopyable 和enable_shared_from_this，保证了代码的复用性。还有比较重要的一个继承就是IOmanger类对协程调度器的继承，既使用了协程调度器的一部功能，又添加了epoll的功能，将两个模块更融洽的结合在一起。

多态：主要是用了静态多态。比如日志系统中对流运算符的重载，以及各种函数模板的泛型编程。

## 项目某些重点模块的细节

#### 协程模块

#### 协程调度模块

#### io协程调度模块

#### hook模块

#### http模块

#### tcp模块



# 中兴项目

## 项目背景

探针项目是公司统一的数据源平台，支持多种网络制式，多种业务分析，多种数据源采集。探针项目是vmax大数据产品，核心网软采，以及其他需要采集核心网数据源的产品提供数据源支撑，开发了解密，视频，重复信令去重，语音mos等功能；

技术上，需要熟悉全网通讯协议，对通讯网络中的原始码流进行采集解码，流量过大，对性能要求过高，涉及网元，接口，协议，项目内部集成了dpi技术；

市场方面，主要集中在通讯网络采集，涵盖2345g，各种制式以及无线侧数据全网采集，和大数据产品vmax配套提供感知，网优，运维，网络检测，安全等领域解决方案；另外也可以作为通讯网络独立数据源产品对接第三方应用。

### 1、新需求的开发：平台代码新增协程模块

#### 需求目的

最终想要实现一个类似于PhotonLibOS协程库的东西，后期和DPDK结合，简化DPDK应用程序的开发，并且增加更多的功能，比如文件IO。**但是现在项目中的DPDK只使用了简单的数据链路层高性能包转发的功能**，为了之后将DPDK支持TCP/IP协议栈，必须进行的两个工作，第一个是用户态TCP/IP协议栈，**第二个是高性能的IO处理过程。**

#### 用户态TCP/IP协议栈：

用户态TCP/IP协议栈：关于开源的一些尝试：一是移植现有的协议栈如FreeBSD，另一种就是完全重写。这些基于DPDK的开源项目的测试数据通常能够获得比Linux内核的原生socket更好的性能。关于DPDK用户态TCP/IP协议栈的开发不是我在进行，我测试协程代码库的时候使用了腾讯开源的F-Stack（基于DPDK并且集成了FreeBSD），集成了 FreeBSD做了一些裁剪，提供了一组POSIX API，如`socket`、`epoll`、`kqueue`等，提供用户态的网络协议栈。看了F-Stack 示例代码，它目前的核心代码仍然是 while 循环的模式，需要提供一个loop，然后注册到大循环内部执行，网卡的收发包代码被安排在了loop的上下文附近。**虽然说已经提供了事件接口（这是协程化的必备条件），但由于没有调度器，所以整体代码风格跟之前的DPDK仍然差不多。**

#### 实现（待补充）

##### 协程封装？



##### 协程调度器设计？



##### io协程调度设计？



##### 验证

我将写的协程调度器与F-Stack的while循环集成到一起，创建协程调度器，协程调度器使用`epoll`作为调度器的主事件引擎，可以注册关注的fd，不管是文件fd还是socket，当I/O事件完成时，对应的协程会被唤醒，并执行后续操作。

#### 测试结果：

使用产品商用版本的测试网卡（100G的）进行测试，服务端单线程，包大小512字节，ping流量类型，主要是针对啊吞吐量进行一个测试，大约可以实现的5gb/s的流量。当时没有和PhotonLibOS库做对比，因为当时没有使用过这个库，开发完成之后可能还需要和用户态TCP/IP协议栈模块完成之后在联调测试。

#### 最大的困难

1、可能还是在于协程的实现以及和后面的DPDK进行联调方面，因为之前没有接触过DPDK，所以学习使用DPDK联调测试这块比较困难。

2、集中在协程调度的实现上，如何更加高效的实现协程之间的一个调度。

#### 后续

后续有时间的话可能1、在关键路径加入汇编代码进一步提高性能 。

#### DPDK

**DPDK**程序在每个CPU核心上运行的函数都是一个大的 while 循环。在循环中，可以添加网卡收发包的代码，以及对应的业务逻辑，整体上的架构是一个异步回调的事件模型。由于是 polling，需要让一次循环尽快结束以便开启下一轮，因此代码中一般不能出现长时间的阻塞调用，如`sleep`。业务逻辑一般只关注网络，比如网关、防火墙这种典型应用，涉及到文件I/O的功能，可能还需要通过跨线程通信的方式，转交给专门的I/O线程去执行。

内核旁路的主要思想是Linux只用来处理控制流；所有数据流都在用户空间中处理。因此，内核旁路可以避免内核数据包复制、线程调度、系统调用和中断带来的性能瓶颈。（DPDK就是一种内核旁路技术）



### 2、招标用户面落盘文件的统计工具的开发：

使用python进行的一个统计工具开发，在项目经过数据采集和事件回填之后，生成大量的话单，在话单入库之前，需要对生成的话单进行统计，包括按照话单用户号段、隧道、网络协议进行筛选，并且计算上下行流量，还有各种回填字段的回填率。

项目数据很大，解压后大约600G左右，由很多个小txt文件组成，每个文件大小在几kb大小不等，每个文件里包含若干行数据，且每个文件的数据长度不一致，甚至会遇到空文件。不了解数据具体情况前，进行统计的时候时候，至少需要遍历一遍所有文件，如果使用pandas的read_csv方法去循环遍历读取，速度非常慢。


##### 实现技术功能：

1. 读取ini配置文件的类，这样可以方便工具使用人员的更加灵活的进行配置路径、进程数、筛选条件以及统计内容等。

2. 设计了多进程处理文件，以便充分利用服务器多CPU的性能。

   多进程方面，可以将所以的文件路径放到一个list中，利用多进程分别处理其中每个文件。

   1. 获取所有文件的路径，去重；
   2. 将文件按照数量平均分配到各个组中；
   3. open方法取代pandas的read_csv方法读取文件，每组内多个文件用多线程处理，同时每个样本组用多进程处理；

##### 技术难点：

- 统计速度，效率

  - 利用多cpu，使用python的multipricessing 库

    python中的多线程无法利用多核优势，如果想要充分的使用多核CPU的资源，在python中大部分情况需要使用多进程。python中提供了 multipricessing 模块多进程-Pool进程池模块，该模块用于开启进程，并在子进程中执行我们定制的任务，比如函数。可以提供指定数量的进程供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。

    ```pytho
    class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])
    ```

    ```python
    # 导入multiprocessing模块
    import multiprocessing
    # 导入multiprocessing模块下的Process类
    from multiprocessing import Process
    ```

    

  - 多进程之间的通信，然后汇总

    1.  进程之间不共享全局变量,当一个进程对全局变量中的数据进行修改，对于其他进程而言，不会造成任何影响，可以理解为每个进程拿的都是最初的全局变量，或者可以理解为全局变量就是所谓的资源，当创建一个进程，则系统会直接给这个进程复制一个全局变量，针对于这个全局变量而言，再进程之间都是相互独立存在的，之间没有任何的关系。
    2. 所以需要统计的全局数据只能通过进程间通信的方式进行处理。多个子进程间的通信就要采用Queue，比如有以下的需求，一个子进程向队列中写数据，另外一个进程从队列中取数据。但是queue使用了1个线程互斥锁(pthread.Lock())，以及3个条件标量(pthread.condition()),来保证了线程安全。这样处理虽然使用了多进程，并且实现了进程间全局变量的通信，但是会导致使用该全局变量的进程对这个变量加锁，其他进程只能等待，实际运行中速度还是会很慢
    3. **最后解决：**干脆放弃进程间通信，每个进程统计自己的数据，最后将所有进程的统计结果进行汇总，虽然最后汇总增加了一下工作量，但是针对要处理的几百G的数据来说，这点处理时间可以忽略不记。实现将5分钟处理150G的数据。

### 3、项目数据采集：

协议分析模块：使用dpdk接管网卡，将码流读取出来，然后进行实时分析，并生成规则的话单。其中要进行码流的解码，还有一些加密数据需要解密等等

事件回填模块：因为协议分析模块生成的话单是用户面的数据，不包括控制面的数据，比如号段、地域、小区等，所以需要该模块将控制面的数据回填到话单中。

每一个模块中间使用一个中转模块xdr，方便使用抓包工具在数据传输过程中进行数据抓取，然后分析数据是否正确。

### 4、安全探针预研	

模块预研，创新压力，需要新增安全模块，最终决定将入侵检测的安全模块加入到项目中。调研了几个开源的入侵检测模块，最终决定使用snort，将其集成到项目中。

##### 原因如下：

1. 具有实时数据流量分析和记录IP网络数据包的能力，能够进行协议分析，对网络数据包内容进行搜索/匹配。
2. 它能够检测各种不同的攻击方式，对攻击进行实时报警。
3. Snort是开源的入侵检测系统，它扩展性和可移植性很好，可以很容易的集成到其他系统中。

snort拥有三大基本功能：嗅探器、数据包记录器和入侵检测。

- 嗅探器模式仅从网络上读取数据包并作为连续不断的流显示在终端上，常用命令snort -dev，公司的探针项目可以支持这部分的工作，并且更加高效。
- 数据包记录器模式是把数据包记录到硬盘上，常用命令snort -b，项目中不需要。
- 网络入侵检测模式是最复杂的，而且是可配置的。我们可以让Snort分析网络数据流以匹配用户定义的一些规则，并根据检测结果采取一定的动作。最终决定将这部分代码模块集成到项目中，在生成话单时，如果检测到码流有风险，就在话单中设置标志位进行判断。

最终实现的功能，将入侵模块集成到了项目数据采集模块之后，进行分析，取消了snort原本的报警模式，取而代之的是将报警类型在话单字段中进行体现。简单设置了一些安全规则，包括告警IP的连接等，然后话单字段设置了一个1位的字段，进行判断是否有安全问题。

后续可能增加字段位数来判断是何种告警以及是否需要进行处理和屏蔽，如何进行屏蔽等、进一步降低对性能的影响。



##### 难点：

1、调研合适的开源组件。需要实时性好、易扩展、支持动态扩展检测规则。

2、集成过程。这部分是在同事的合作下共同完成的，因为代码需要基于自己平台库和函数进行开发，有很多函数不熟悉，需要其他同事的帮助。





## NFC项目

1、NFC运行原理，**NFC驱动开发**，钱包升级、刷卡异常率、

2、学习相关协议模型

3、集成到穿戴设备中



## TP-LINK

完成项目的jenkins配合docker容器化编译的工作，实现代码速度上报以及精细化度量的Devops任务上报

使用wireshark以及内部抓包工具对涉及到的相关通信协议字段进行抓取，解读，分析

使用C++编写相关工具类对某段特性数据进行识别，然后过滤





